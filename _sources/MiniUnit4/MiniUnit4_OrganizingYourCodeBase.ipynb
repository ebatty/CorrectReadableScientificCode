{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyObCPnvqkHIfY0hv2e05alY"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mini-unit #4: Organizing your code base"
   ],
   "metadata": {
    "id": "DiIZYum-g_o5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Video\n",
    "\n",
    "Hand-written notes with voice-over\n",
    "\n",
    "Script:\n",
    "\n",
    "Typically, you will want to create one code base per research project. For example, often people have one code base per paper.\n",
    "\n",
    "While working through the exercises in the previous mini-unit, you may have noticed that you were writing functions AND a main script that called those functions. This corresponds to a conceptual divide in your code between low-level and high-level code.\n",
    "\n",
    "Low-level code consists of all of your functions, doing things from loading data to plotting to analyzing properties of the data. This low-level code is called source code. There should not be jupyter notebooks in your low-level source code.\n",
    "\n",
    "High-level code consists of the scripts that call those functions (and doesn't define new ones). For example, these will often string functions together into analysis pipelines run on specific data. The high-level code is often called scripts.\n",
    "\n",
    "Let's dive a little more into how to organize these two parts of your code. The low-level source code is typically contained in a folder called `src` or (somewhat confusingly) named after the overal code repo. So you might have a code repo called `neuralencode` with a folder called `neuralencode` that contains the source code.\n",
    "\n",
    "Within that folder, you want to organize related functions into different files with a useful file name. For example, you might place all functions that compute a loss function (for training models) into a file called `loss_functions.py`. Or all functions that preprocess data into a file called `data_preprocessing.py`. People often make a bit of a catch-all file called `utils.py` that has functions that are very low-level and do not fit elsewhere.\n",
    "\n",
    "Within each file, you can get even more organized! If there is a function that is slightly higher-level than the rest and calls other functions, you should place that at the top of the file, with the called functions below it.\n",
    "\n",
    "So what about the scripts? These should be placed into a scripts folder. These scripts could be `.py` files or sometimes jupyter notebooks (`.ipynb` files).\n"
   ],
   "metadata": {
    "id": "GlMBIqBXhFa-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise\n",
    "\n",
    "The best way to improve your code is to learn from others. Take some time to look through these well-organized repos:\n",
    "- https://github.com/michaelerule/glm_moment_closure\n",
    "- https://github.com/jbohnslav/deepethogram\n",
    "\n",
    "For each, reflect on or discuss the following questions:\n",
    "\n",
    "- What is helpful about how it is structured? What is the source and what is the script?\n",
    "- What function groupings do they have in different files?\n",
    "\n",
    "<textarea name=\"comments\" id=\"comments\">\n",
    "...\n",
    "</textarea><br />\n",
    "<input type=\"submit\" value=\"Submit\" />"
   ],
   "metadata": {
    "id": "oRbb9301hYEZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Coding exercise: structuring analysis pipeline\n",
    "\n",
    "Let's build on the example from the last mini-unit. You've already organized a long script into different functions. Now, think through how you would structure the functions into different files.\n",
    "\n",
    "If you are able, open up VSCode and actually create folders called src and scripts in a new project. Copy and paste the functions into new files and the script into a main script file. If you are not able, write out below what each file in `src` would be named."
   ],
   "metadata": {
    "id": "F8JZk4Jph8tc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Generate data\n",
    "spikes, trial_angles, reaching_angles = load_data()\n",
    "\n",
    "## Plot rasters for example neuron\n",
    "example_neuron_index = 1\n",
    "plot_spike_rasters(spikes, trial_angles)\n",
    "  # within this function spike_times = convert_to_spike_times(spikes, trial_angles)\n",
    "\n",
    "### Compute tuning of each neuron\n",
    "n_neurons = 8\n",
    "tuning_indices = np.zeros((n_neurons,))\n",
    "preferred_orientations = np.zeros((n_neurons,))\n",
    "for i_neuron in range(1):\n",
    "\n",
    "    # Compute average firing rate per angle\n",
    "    firing_rate_per_angle = compute_firing_rate_per_angle(spikes[i_neuron], trial_angles)\n",
    "\n",
    "    # Get preferred orientation\n",
    "    preferred_orientations[i_neuron] = reaching_angles[np.argmax(firing_rate_per_angle)]\n",
    "\n",
    "    # Compute tuning index\n",
    "    tuning_indices[i_neuron] = compute_tuning_index(firing_rate_per_angle)\n",
    "\n",
    "plot_tuning_indices(tuning_indices)\n",
    "plot_preferred_angle(tuning_indices, preferred_orientations)"
   ],
   "metadata": {
    "id": "OdmvGrGg_fKN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<details>\n",
    "<summary> <font color='blue'>Click here for solution explanation </font></summary>\n",
    "\n",
    "There will be solution video here when I live code.\n",
    "\n",
    "Files:\n",
    "\n",
    "plotting.py with `plot_spike_rasters`, `plot_tuning_indices`, and `plot_preferred_angle`\n",
    "\n",
    "data_loading.py with `load_data`\n",
    "\n",
    "data_preprocessing.py with `convert_to_spike_times`\n",
    "\n",
    "compute_neural_properties.py with `compute_firing_rate_per_angle`, and `compute_tuning_index`\n",
    "\n",
    "</details>\n"
   ],
   "metadata": {
    "id": "s4QL-JND_o9x"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Video\n",
    "\n",
    "We have two folders in our code base: src and scripts. What other folders should we have?\n",
    "\n",
    "There are a few different templates for scientific projects in Python. My favorite is one that Patrick Mineault developed based on several of these.\n",
    "\n",
    "In addition to src and scripts folders, Patrick recommends having:\n",
    "- a data folder, where you put the raw data for your project.\n",
    "- a docs folder where you place documentation\n",
    "- a results folder where you place results your code has computed\n",
    "- a tests folder where you place tests for your code (we'll cover this in the next mini-unit).\n",
    "\n",
    "You can create these folders yourself but Patrick has a tool you can use to automatically create this structure. You can call `cookiecutter gh:patrickmineault/true-neutral-cookiecutter` to initialize an empty repo with this structure. [demo this live]\n",
    "\n",
    "You may notice a difficulty here: Python functions are notoriously difficult to import from other folders. So it can get convoluted to import functions from your source. Let's look at an example. [live demo]\n",
    "\n",
    "Luckily, there is a fix that seems really complicated but is actually surprisingly simple. You can make your code repo into a locally pip installable package. This means that you can import functions from source using syntax similar to `import numpy as np`. I'll link to full instructions on how to create a locally pip installable package, but luckily Patrick has removed much of the work.\n",
    "\n",
    "When we used the cookiecutter command, we created some extra files, including a `setup.py` file and an `__init__.py` file. Now, all we have to do is install the package by using\n",
    "`pip install -e`. We do not need to re-install even if we change files inside `src`. Let's see how this fixed our import issue [live demo]."
   ],
   "metadata": {
    "id": "8hC8M8IkiEUN"
   }
  }
 ]
}